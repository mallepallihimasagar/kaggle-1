{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:865: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\lda.py:6: DeprecationWarning: lda.LDA has been moved to discriminant_analysis.LinearDiscriminantAnalysis in 0.17 and will be removed in 0.19\n",
      "  \"in 0.17 and will be removed in 0.19\", DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\learning_curve.py:23: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the functions are moved. This module will be removed in 0.20\n",
      "  DeprecationWarning)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\qda.py:6: DeprecationWarning: qda.QDA has been moved to discriminant_analysis.QuadraticDiscriminantAnalysis in 0.17 and will be removed in 0.19.\n",
      "  \"in 0.17 and will be removed in 0.19.\", DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import lightgbm as lgb\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.model_selection import KFold, train_test_split\n",
    "from sklearn import *\n",
    "from scipy import *\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "import nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Load the list of documents\n",
    "train_var = pd.read_csv(\"training_variants\")\n",
    "test_var = pd.read_csv(\"test_variants\")\n",
    "\n",
    "# Reading train\n",
    "train_text = pd.read_csv(\n",
    "    'training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, \n",
    "    names=[\"ID\",\"Text\"])\n",
    "#read test\n",
    "test_text = pd.read_csv(\n",
    "    'test_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, \n",
    "    names=[\"ID\",\"Text\"])\n",
    "\n",
    "#merge text and variants\n",
    "train = pd.merge(train_var,train_text)\n",
    "test = pd.merge(test_var,test_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3260\n",
      "3091\n"
     ]
    }
   ],
   "source": [
    "#gen_var_lst = sorted(list(train.Gene.unique()) + list(train.Variation.unique()))\n",
    "#print(len(gen_var_lst))\n",
    "#gen_var_lst = [x for x in gen_var_lst if len(x.split(' '))==1]\n",
    "#print(len(gen_var_lst))\n",
    "#i_ = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_all = pd.concat((train, test), axis=0, ignore_index=True)\n",
    "df_all['Gene_Share'] = df_all.apply(lambda r: sum([1 for w in r['Gene'].split(' ') if w in r['Text'].split(' ')]), axis=1)\n",
    "df_all['Variation_Share'] = df_all.apply(lambda r: sum([1 for w in r['Variation'].split(' ') if w in r['Text'].split(' ')]), axis=1)\n",
    "\n",
    "for c in df_all.columns:\n",
    "    if df_all[c].dtype == 'object':\n",
    "        if c in ['Gene','Variation']:\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            df_all[c+'_lbl_enc'] = lbl.fit_transform(df_all[c].values)  \n",
    "            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n",
    "            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' ')))\n",
    "        elif c != 'Text':\n",
    "            lbl = preprocessing.LabelEncoder()\n",
    "            df_all[c] = lbl.fit_transform(df_all[c].values)\n",
    "        if c=='Text': \n",
    "            df_all[c+'_len'] = df_all[c].map(lambda x: len(str(x)))\n",
    "            df_all[c+'_words'] = df_all[c].map(lambda x: len(str(x).split(' '))) \n",
    "\n",
    "train = df_all.iloc[:len(train)]\n",
    "test = df_all.iloc[len(train):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data treatment\n",
    "X_train = train['Text']\n",
    "X_test = test['Text']\n",
    "y_train = train['Class']-1\n",
    "all_data = pd.concat((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#data split for cv\n",
    "#X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train, y_train, test_size=0.33, random_state=201612)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#for CV\n",
    "#simple method\n",
    "#all_data = pd.concat((X_train,X_test))\n",
    "#vect2 = CountVectorizer(min_df = 5).fit(all_data)\n",
    "vect2 = CountVectorizer(min_df = 5, ngram_range=(1,8), max_features=500).fit(all_data)\n",
    "#vect2 = TfidfVectorizer(min_df= 5,ngram_range=(1,3)).fit(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#X_train_vect2_cv = vect2.transform(X_train_cv).toarray()\n",
    "#X_test_vect2_cv = vect2.transform(X_test_cv).toarray()\n",
    "#X_test_vect_lgbm = vect2.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vcount vectorize for cv loop using X_train\n",
    "vect2 = CountVectorizer(min_df = 5, ngram_range=(1,8), max_features=500).fit(all_data)\n",
    "X_train_vect2_cvloop = vect2.transform(X_train)#.toarray()\n",
    "X_test_vect_lgbm = vect2.transform(X_test)#.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Gene_lbl_enc'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Gene_len'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Gene_words'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Gene_Share'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Variation_lbl_enc'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Variation_len'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Variation_words'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Variation_Share'])\n",
    "X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Text_len'])\n",
    "#X_train_vect2_cvloop = add_feature(X_train_vect2_cvloop, train['Text_words'])\n",
    "#X_test_vect_lgbm\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Gene_lbl_enc'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Gene_len'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Gene_words'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Gene_Share'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Variation_lbl_enc'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Variation_len'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Variation_words'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Variation_Share'])\n",
    "X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Text_len'])\n",
    "#X_test_vect_lgbm = add_feature(X_test_vect_lgbm, test['Text_words'])\n",
    "# convert to array\n",
    "X_train_vect2_cvloop = X_train_vect2_cvloop.toarray()\n",
    "X_test_vect_lgbm = X_test_vect_lgbm.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#svdT = TruncatedSVD(n_components=30)\n",
    "#svdTFit = svdT.fit(X_train_vect2_cvloop)\n",
    "#svdtest = svdT.transform(X_test_vect_lgbm)\n",
    "#svdtrain = svdT.transform(X_train_vect2_cvloop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:100: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 1.43433\n",
      "[100]\tvalid_0's multi_logloss: 1.19158\n",
      "[150]\tvalid_0's multi_logloss: 1.08006\n",
      "[200]\tvalid_0's multi_logloss: 1.02213\n",
      "[250]\tvalid_0's multi_logloss: 0.992197\n",
      "[300]\tvalid_0's multi_logloss: 0.979258\n",
      "[350]\tvalid_0's multi_logloss: 0.973452\n",
      "Early stopping, best iteration is:\n",
      "[375]\tvalid_0's multi_logloss: 0.971955\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 1.41422\n",
      "[100]\tvalid_0's multi_logloss: 1.17813\n",
      "[150]\tvalid_0's multi_logloss: 1.07522\n",
      "[200]\tvalid_0's multi_logloss: 1.02272\n",
      "[250]\tvalid_0's multi_logloss: 1.00569\n",
      "[300]\tvalid_0's multi_logloss: 0.997833\n",
      "[350]\tvalid_0's multi_logloss: 0.993501\n",
      "Early stopping, best iteration is:\n",
      "[330]\tvalid_0's multi_logloss: 0.991972\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 1.40019\n",
      "[100]\tvalid_0's multi_logloss: 1.14322\n",
      "[150]\tvalid_0's multi_logloss: 1.03111\n",
      "[200]\tvalid_0's multi_logloss: 0.971676\n",
      "[250]\tvalid_0's multi_logloss: 0.936991\n",
      "[300]\tvalid_0's multi_logloss: 0.919193\n",
      "[350]\tvalid_0's multi_logloss: 0.909792\n",
      "Early stopping, best iteration is:\n",
      "[368]\tvalid_0's multi_logloss: 0.909058\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 1.41304\n",
      "[100]\tvalid_0's multi_logloss: 1.16872\n",
      "[150]\tvalid_0's multi_logloss: 1.05629\n",
      "[200]\tvalid_0's multi_logloss: 0.996914\n",
      "[250]\tvalid_0's multi_logloss: 0.969326\n",
      "[300]\tvalid_0's multi_logloss: 0.959322\n",
      "[350]\tvalid_0's multi_logloss: 0.95497\n",
      "Early stopping, best iteration is:\n",
      "[333]\tvalid_0's multi_logloss: 0.954811\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[50]\tvalid_0's multi_logloss: 1.38686\n",
      "[100]\tvalid_0's multi_logloss: 1.11966\n",
      "[150]\tvalid_0's multi_logloss: 0.992941\n",
      "[200]\tvalid_0's multi_logloss: 0.930209\n",
      "[250]\tvalid_0's multi_logloss: 0.892535\n",
      "[300]\tvalid_0's multi_logloss: 0.87376\n",
      "[350]\tvalid_0's multi_logloss: 0.865765\n",
      "[400]\tvalid_0's multi_logloss: 0.861825\n",
      "[450]\tvalid_0's multi_logloss: 0.861676\n",
      "Early stopping, best iteration is:\n",
      "[436]\tvalid_0's multi_logloss: 0.860803\n"
     ]
    }
   ],
   "source": [
    "# LightGBM parameters\n",
    "denom = 0\n",
    "fold = 5 #Change to 5, 1 for Kaggle Limits\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'metric': {'multi_logloss'},\n",
    "        'num_class': 9,\n",
    "        'learning_rate': 0.025,\n",
    "        'max_depth': 15,\n",
    "        'num_leaves': 95,\n",
    "        'min_data_in_leaf': 80,\n",
    "        'lambda_l1': 1.0,\n",
    "        'num_iteration': 2000,\n",
    "        'feature_fraction': 0.9, \n",
    "        'bagging_fraction': 0.9, \n",
    "        'bagging_freq': 5\n",
    "        #'verbose': 0\n",
    "    }\n",
    "    X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train_vect2_cvloop, y_train, test_size=0.20, random_state=i)\n",
    "    #X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(svdtrain, y_train, test_size=0.18, random_state=i)\n",
    "    #X_train_vect2_cv = vect2.transform(X_train_cv).toarray()\n",
    "    #X_test_vect2_cv = vect2.transform(X_test_cv).toarray()\n",
    "\n",
    "    #lgb_train = lgb.Dataset(X_train_vect2_cv, y_train_cv)\n",
    "    #lgb_eval = lgb.Dataset(X_test_vect2_cv, y_test_cv, reference=lgb_train)\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_cv, y_train_cv)\n",
    "    lgb_eval = lgb.Dataset(X_test_cv, y_test_cv, reference=lgb_train)\n",
    "\n",
    "# train\n",
    "    gbm = lgb.train(params,\n",
    "            lgb_train,\n",
    "            #num_boost_round=rnds,\n",
    "            valid_sets=lgb_eval,\n",
    "            verbose_eval=50,\n",
    "            early_stopping_rounds=20)\n",
    "    #score1 = metrics.log_loss(y_test_cv, gbm.predict(X_test_cv, num_iteration=gbm.best_iteration), labels = list(range(9)))\n",
    "    #if score < 0.9:\n",
    "    if denom != 0:\n",
    "        pred = gbm.predict(X_test_vect_lgbm, num_iteration=gbm.best_iteration+50)\n",
    "        #pred = gbm.predict(svdtest, num_iteration=gbm.best_iteration+50)\n",
    "        \n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = gbm.predict(X_test_vect_lgbm, num_iteration=gbm.best_iteration+50)\n",
    "        #pred = gbm.predict(svdtest, num_iteration=gbm.best_iteration+50)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    submission = submission.reset_index()\n",
    "    submission.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "    #submission['ID'] = pid\n",
    "    submission.to_csv('submission_xgb_fold_'  + str(i) + '.csv', index=False)\n",
    "    \n",
    "preds /= denom\n",
    "submission = pd.DataFrame(preds, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission = submission.reset_index()\n",
    "submission.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "#submission['ID'] = pid\n",
    "submission.to_csv(\"sub_LGBM.csv\", index=False)\n",
    "#y_pred = gbm.predict(X_test_vect_lgbm, num_iteration=gbm.best_iteration)\n",
    "#y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=y_pred[0:,0:], columns=[\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"])\n",
    "df = df.reset_index()\n",
    "df.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "df.to_csv(\"sub_5_LGBM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#TFIDF\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "vect_tfidf = TfidfVectorizer().fit(all_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_vect_tfidf_cv = vect_tfidf.transform(X_train_cv).toarray()\n",
    "X_test_vect_tfidf_cv = vect_tfidf.transform(X_test_cv).toarray()\n",
    "X_test_vect_tfidf_lgbm = vect_tfidf.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:100: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\tvalid_0's multi_logloss: 2.02684\n",
      "Training until validation scores don't improve for 10 rounds.\n",
      "[2]\tvalid_0's multi_logloss: 1.89971\n",
      "[3]\tvalid_0's multi_logloss: 1.79927\n",
      "[4]\tvalid_0's multi_logloss: 1.71384\n",
      "[5]\tvalid_0's multi_logloss: 1.641\n",
      "[6]\tvalid_0's multi_logloss: 1.57715\n",
      "[7]\tvalid_0's multi_logloss: 1.52139\n",
      "[8]\tvalid_0's multi_logloss: 1.47396\n",
      "[9]\tvalid_0's multi_logloss: 1.4317\n",
      "[10]\tvalid_0's multi_logloss: 1.39493\n",
      "[11]\tvalid_0's multi_logloss: 1.36257\n",
      "[12]\tvalid_0's multi_logloss: 1.33521\n",
      "[13]\tvalid_0's multi_logloss: 1.30837\n",
      "[14]\tvalid_0's multi_logloss: 1.28498\n",
      "[15]\tvalid_0's multi_logloss: 1.26408\n",
      "[16]\tvalid_0's multi_logloss: 1.24439\n",
      "[17]\tvalid_0's multi_logloss: 1.22785\n",
      "[18]\tvalid_0's multi_logloss: 1.21154\n",
      "[19]\tvalid_0's multi_logloss: 1.19687\n",
      "[20]\tvalid_0's multi_logloss: 1.18486\n",
      "[21]\tvalid_0's multi_logloss: 1.17331\n",
      "[22]\tvalid_0's multi_logloss: 1.16393\n",
      "[23]\tvalid_0's multi_logloss: 1.15552\n",
      "[24]\tvalid_0's multi_logloss: 1.14686\n",
      "[25]\tvalid_0's multi_logloss: 1.14038\n",
      "[26]\tvalid_0's multi_logloss: 1.13504\n",
      "[27]\tvalid_0's multi_logloss: 1.12926\n",
      "[28]\tvalid_0's multi_logloss: 1.12494\n",
      "[29]\tvalid_0's multi_logloss: 1.12266\n",
      "[30]\tvalid_0's multi_logloss: 1.11913\n",
      "[31]\tvalid_0's multi_logloss: 1.11634\n",
      "[32]\tvalid_0's multi_logloss: 1.11267\n",
      "[33]\tvalid_0's multi_logloss: 1.11102\n",
      "[34]\tvalid_0's multi_logloss: 1.11029\n",
      "[35]\tvalid_0's multi_logloss: 1.10825\n",
      "[36]\tvalid_0's multi_logloss: 1.10731\n",
      "[37]\tvalid_0's multi_logloss: 1.1065\n",
      "[38]\tvalid_0's multi_logloss: 1.10522\n",
      "[39]\tvalid_0's multi_logloss: 1.10504\n",
      "[40]\tvalid_0's multi_logloss: 1.10527\n",
      "[41]\tvalid_0's multi_logloss: 1.10644\n",
      "[42]\tvalid_0's multi_logloss: 1.10786\n",
      "[43]\tvalid_0's multi_logloss: 1.10963\n",
      "[44]\tvalid_0's multi_logloss: 1.1105\n",
      "[45]\tvalid_0's multi_logloss: 1.11239\n",
      "[46]\tvalid_0's multi_logloss: 1.11439\n",
      "[47]\tvalid_0's multi_logloss: 1.11693\n",
      "[48]\tvalid_0's multi_logloss: 1.11851\n",
      "[49]\tvalid_0's multi_logloss: 1.12054\n",
      "Early stopping, best iteration is:\n",
      "[39]\tvalid_0's multi_logloss: 1.10504\n"
     ]
    }
   ],
   "source": [
    "lgb_train = lgb.Dataset(X_train_vect_tfidf_cv, y_train_cv)\n",
    "lgb_eval = lgb.Dataset(X_test_vect_tfidf_cv, y_test_cv, reference=lgb_train)\n",
    "\n",
    "# LightGBM parameters\n",
    "params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'metric': {'multi_logloss'},\n",
    "        'num_class': 9,\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 23,\n",
    "        'min_data_in_leaf': 1,\n",
    "        'num_iteration': 100,\n",
    "        'verbose': 0\n",
    "}\n",
    "\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "            lgb_train,\n",
    "            num_boost_round=100,\n",
    "            valid_sets=lgb_eval,\n",
    "            early_stopping_rounds=10)\n",
    "y_pred = gbm.predict(X_test_vect_tfidf_lgbm, num_iteration=gbm.best_iteration)\n",
    "#y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_test_vect_tfidf_lgbm, num_iteration=gbm.best_iteration)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=y_pred[0:,0:], columns=[\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"])\n",
    "df = df.reset_index()\n",
    "df.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "df.to_csv(\"sub_6_LGBM_TFIDF.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ngram vectorize for cv loop using X_train\n",
    "vect_ngram = CountVectorizer(min_df = 5, analyzer= 'char_wb',ngram_range=(1,3)).fit(X_train)\n",
    "X_train_vect2_cvloop_ngram = vect_ngram.transform(X_train).toarray()\n",
    "#ngram for prediction using X_test\n",
    "X_test_vect_ngram_lgbm = vect_ngram.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\lightgbm\\engine.py:100: UserWarning: Found `num_iteration` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training until validation scores don't improve for 20 rounds.\n",
      "[20]\tvalid_0's multi_logloss: 1.73876\n",
      "[40]\tvalid_0's multi_logloss: 1.50357\n",
      "[60]\tvalid_0's multi_logloss: 1.35862\n",
      "[80]\tvalid_0's multi_logloss: 1.25938\n",
      "[100]\tvalid_0's multi_logloss: 1.18667\n",
      "[120]\tvalid_0's multi_logloss: 1.13301\n",
      "[140]\tvalid_0's multi_logloss: 1.10008\n",
      "[160]\tvalid_0's multi_logloss: 1.07369\n",
      "[180]\tvalid_0's multi_logloss: 1.05735\n",
      "[200]\tvalid_0's multi_logloss: 1.04176\n",
      "[220]\tvalid_0's multi_logloss: 1.02958\n",
      "[240]\tvalid_0's multi_logloss: 1.02021\n",
      "[260]\tvalid_0's multi_logloss: 1.01678\n",
      "[280]\tvalid_0's multi_logloss: 1.01456\n",
      "[300]\tvalid_0's multi_logloss: 1.01286\n",
      "Early stopping, best iteration is:\n",
      "[297]\tvalid_0's multi_logloss: 1.01229\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[20]\tvalid_0's multi_logloss: 1.73951\n",
      "[40]\tvalid_0's multi_logloss: 1.49942\n",
      "[60]\tvalid_0's multi_logloss: 1.34923\n",
      "[80]\tvalid_0's multi_logloss: 1.24726\n",
      "[100]\tvalid_0's multi_logloss: 1.18101\n",
      "[120]\tvalid_0's multi_logloss: 1.13119\n",
      "[140]\tvalid_0's multi_logloss: 1.09805\n",
      "[160]\tvalid_0's multi_logloss: 1.07297\n",
      "[180]\tvalid_0's multi_logloss: 1.05642\n",
      "[200]\tvalid_0's multi_logloss: 1.04839\n",
      "[220]\tvalid_0's multi_logloss: 1.04334\n",
      "[240]\tvalid_0's multi_logloss: 1.03859\n",
      "[260]\tvalid_0's multi_logloss: 1.03803\n",
      "[280]\tvalid_0's multi_logloss: 1.03809\n",
      "[300]\tvalid_0's multi_logloss: 1.03801\n",
      "Early stopping, best iteration is:\n",
      "[290]\tvalid_0's multi_logloss: 1.03658\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[20]\tvalid_0's multi_logloss: 1.70666\n",
      "[40]\tvalid_0's multi_logloss: 1.45066\n",
      "[60]\tvalid_0's multi_logloss: 1.29043\n",
      "[80]\tvalid_0's multi_logloss: 1.18172\n",
      "[100]\tvalid_0's multi_logloss: 1.10705\n",
      "[120]\tvalid_0's multi_logloss: 1.05457\n",
      "[140]\tvalid_0's multi_logloss: 1.01477\n",
      "[160]\tvalid_0's multi_logloss: 0.984241\n",
      "[180]\tvalid_0's multi_logloss: 0.96172\n",
      "[200]\tvalid_0's multi_logloss: 0.947106\n",
      "[220]\tvalid_0's multi_logloss: 0.933147\n",
      "[240]\tvalid_0's multi_logloss: 0.925716\n",
      "[260]\tvalid_0's multi_logloss: 0.919422\n",
      "[280]\tvalid_0's multi_logloss: 0.91618\n",
      "[300]\tvalid_0's multi_logloss: 0.91218\n",
      "[320]\tvalid_0's multi_logloss: 0.911281\n",
      "[340]\tvalid_0's multi_logloss: 0.91183\n",
      "Early stopping, best iteration is:\n",
      "[330]\tvalid_0's multi_logloss: 0.910583\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[20]\tvalid_0's multi_logloss: 1.73821\n",
      "[40]\tvalid_0's multi_logloss: 1.50136\n",
      "[60]\tvalid_0's multi_logloss: 1.35612\n",
      "[80]\tvalid_0's multi_logloss: 1.25585\n",
      "[100]\tvalid_0's multi_logloss: 1.18632\n",
      "[120]\tvalid_0's multi_logloss: 1.13546\n",
      "[140]\tvalid_0's multi_logloss: 1.09967\n",
      "[160]\tvalid_0's multi_logloss: 1.0733\n",
      "[180]\tvalid_0's multi_logloss: 1.05603\n",
      "[200]\tvalid_0's multi_logloss: 1.04253\n",
      "[220]\tvalid_0's multi_logloss: 1.03033\n",
      "[240]\tvalid_0's multi_logloss: 1.0261\n",
      "[260]\tvalid_0's multi_logloss: 1.02404\n",
      "Early stopping, best iteration is:\n",
      "[246]\tvalid_0's multi_logloss: 1.02366\n",
      "Training until validation scores don't improve for 20 rounds.\n",
      "[20]\tvalid_0's multi_logloss: 1.71816\n",
      "[40]\tvalid_0's multi_logloss: 1.47122\n",
      "[60]\tvalid_0's multi_logloss: 1.31063\n",
      "[80]\tvalid_0's multi_logloss: 1.20044\n",
      "[100]\tvalid_0's multi_logloss: 1.1242\n",
      "[120]\tvalid_0's multi_logloss: 1.06789\n",
      "[140]\tvalid_0's multi_logloss: 1.02385\n",
      "[160]\tvalid_0's multi_logloss: 0.9922\n",
      "[180]\tvalid_0's multi_logloss: 0.970406\n",
      "[200]\tvalid_0's multi_logloss: 0.95623\n",
      "[220]\tvalid_0's multi_logloss: 0.939815\n",
      "[240]\tvalid_0's multi_logloss: 0.931172\n",
      "[260]\tvalid_0's multi_logloss: 0.923145\n",
      "[280]\tvalid_0's multi_logloss: 0.916178\n",
      "[300]\tvalid_0's multi_logloss: 0.911122\n",
      "[320]\tvalid_0's multi_logloss: 0.910504\n",
      "Early stopping, best iteration is:\n",
      "[305]\tvalid_0's multi_logloss: 0.910504\n"
     ]
    }
   ],
   "source": [
    "# ngram LightGBM parameters\n",
    "denom = 0\n",
    "fold = 5 #Change to 5, 1 for Kaggle Limits\n",
    "for i in range(fold):\n",
    "    params = {\n",
    "        'task': 'train',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'objective': 'multiclass',\n",
    "        'metric': {'multi_logloss'},\n",
    "        'num_class': 9,\n",
    "        'learning_rate': 0.025,\n",
    "        'max_depth': 10,\n",
    "        'num_leaves': 95,\n",
    "        'min_data_in_leaf': 60,\n",
    "        'lambda_l1': 1.0,\n",
    "        'num_iteration': 400,\n",
    "        'feature_fraction': 0.8, \n",
    "        'bagging_fraction': 0.8, \n",
    "        'bagging_freq': 5\n",
    "        #'verbose': 0\n",
    "    }\n",
    "    X_train_cv, X_test_cv, y_train_cv, y_test_cv = train_test_split(X_train_vect2_cvloop_ngram, y_train, test_size=0.18, random_state=i)\n",
    "    #X_train_vect2_cv = vect2.transform(X_train_cv).toarray()\n",
    "    #X_test_vect2_cv = vect2.transform(X_test_cv).toarray()\n",
    "\n",
    "    #lgb_train = lgb.Dataset(X_train_vect2_cv, y_train_cv)\n",
    "    #lgb_eval = lgb.Dataset(X_test_vect2_cv, y_test_cv, reference=lgb_train)\n",
    "    \n",
    "    lgb_train = lgb.Dataset(X_train_cv, y_train_cv)\n",
    "    lgb_eval = lgb.Dataset(X_test_cv, y_test_cv, reference=lgb_train)\n",
    "\n",
    "# train\n",
    "    gbm = lgb.train(params,\n",
    "            lgb_train,\n",
    "            #num_boost_round=rnds,\n",
    "            valid_sets=lgb_eval,\n",
    "            verbose_eval=20,\n",
    "            early_stopping_rounds=20)\n",
    "    #score1 = metrics.log_loss(y_test_cv, gbm.predict(X_test_cv, num_iteration=gbm.best_iteration), labels = list(range(9)))\n",
    "    #if score < 0.9:\n",
    "    if denom != 0:\n",
    "        pred = gbm.predict(X_test_vect_ngram_lgbm, num_iteration=gbm.best_iteration+80)\n",
    "        preds += pred\n",
    "    else:\n",
    "        pred = gbm.predict(X_test_vect_ngram_lgbm, num_iteration=gbm.best_iteration+80)\n",
    "        preds = pred.copy()\n",
    "    denom += 1\n",
    "    submission = pd.DataFrame(pred, columns=['class'+str(c+1) for c in range(9)])\n",
    "    submission = submission.reset_index()\n",
    "    submission.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "    #submission['ID'] = pid\n",
    "    submission.to_csv('submission_lgb_fold_ngram'  + str(i) + '.csv', index=False)\n",
    "    \n",
    "preds /= denom\n",
    "submission = pd.DataFrame(preds, columns=['class'+str(c+1) for c in range(9)])\n",
    "submission = submission.reset_index()\n",
    "submission.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "#submission['ID'] = pid\n",
    "submission.to_csv(\"sub_lgb_ngram.csv\", index=False)\n",
    "#y_pred = gbm.predict(X_test_vect_lgbm, num_iteration=gbm.best_iteration)\n",
    "#y_pred = np.argmax(y_pred, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#ngram\n",
    "#vect_ngram = CountVectorizer(min_df = 5, analyzer= 'char_wb',ngram_range=(2,3)).fit(X_train)\n",
    "#X_train_vect_ngram_cv = vect_ngram.transform(X_train_cv).toarray()\n",
    "#X_test_vect_ngram_cv = vect_ngram.transform(X_test_cv).toarray()\n",
    "#X_test_vect_ngram_lgbm = vect_ngram.transform(X_test).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#vect = CountVectorizer(min_df = 5, analyzer= 'char_wb',ngram_range=(2,5)).fit(X_train)\n",
    "#X_train_vect = vect.transform(X_train)\n",
    "#X_test_vect = vect.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def add_feature(X, feature_to_add):\n",
    "    \"\"\"\n",
    "    Returns sparse feature matrix with added feature.\n",
    "    feature_to_add can also be a list of features.\n",
    "    \"\"\"\n",
    "    from scipy.sparse import csr_matrix, hstack\n",
    "    return hstack([X, csr_matrix(feature_to_add).T], 'csr')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#add features\n",
    "#とりあえずADDしないでやってみる\n",
    "leng = []\n",
    "for i in train['Gene']:\n",
    "    ln = i\n",
    "    leng.append(ln)\n",
    "leng2 = []\n",
    "for i in test['Gene']:\n",
    "    ln = i\n",
    "    leng2.append(ln)\n",
    "\n",
    "\n",
    "#X_train_vect = add_feature(X_train_vect, leng)\n",
    "#X_test_vect = add_feature(X_test_vect, leng2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\base.py:352: RuntimeWarning: overflow encountered in exp\n",
      "  np.exp(prob, prob)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "model = LogisticRegression(C=100).fit(X_train_vect,y_train)\n",
    "pred = model.predict_proba(X_test_vect)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(data=pred[0:,0:], columns=[\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"])  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df = df.reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df.to_csv(\"sub_1.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    " n1 = vect.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_data = pd.concat((X_train,X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#simple method\n",
    "vect2 = CountVectorizer(min_df = 50).fit(all_data)\n",
    "X_train_vect2 = vect2.transform(X_train)\n",
    "X_test_vect2 = vect2.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3321,)"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model2 = MultinomialNB(alpha=0.1).fit(X_train_vect2, y_train)\n",
    "pred2 = model2.predict_proba(X_test_vect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df2 = pd.DataFrame(data=pred2[0:,0:], columns=[\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"])\n",
    "df2 = df2.reset_index()\n",
    "df2.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "df2.to_csv(\"sub_2_MNB.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "model_svc = SVC(C=10000,probability = True).fit(X_train_vect2, y_train)\n",
    "pred_svc = model_svc.predict_proba(X_test_vect2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_svc = pd.DataFrame(data=pred_svc[0:,0:], columns=[\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"])\n",
    "df_svc = df_svc.reset_index()\n",
    "df_svc.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "df_svc.to_csv(\"sub_3_SVC.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "X_train_vect_GBM = vect2.transform(X_train).toarray()\n",
    "X_test_vect_GBM = vect2.transform(X_test).toarray() # Update line\n",
    "model_GBM = GBC(n_estimators=100,max_depth=3).fit(X_train_vect_GBM, y_train)\n",
    "pred_GBM = model_GBM.predict_proba(X_test_vect_GBM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "df_GBM = pd.DataFrame(data=pred_GBM[0:,0:], columns=[\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"])\n",
    "df_GBM = df_GBM.reset_index()\n",
    "df_GBM.columns = [\"ID\",\"class1\",\"class2\",\"class3\",\"class4\",\"class5\",\"class6\",\"class7\",\"class8\",\"class9\"]\n",
    "df_GBM.to_csv(\"sub_4_GBM.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#from sklearn.metrics import mean_squared_error\n",
    "#Gradient Boosting\n",
    "from sklearn.ensemble import GradientBoostingClassifier as GBC\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import random\n",
    "\n",
    "random.seed(1)\n",
    "MSE_list_GB = []\n",
    "dflist_GB = []\n",
    "def GB():\n",
    "    for ne in [100,200,300,400,500]:\n",
    "        for md in [1,2,3,4,5]:\n",
    "            model_gb = GBC(n_estimators=ne,max_depth=md)\n",
    "            MSE = (-cross_val_score(model_gb, X_train_vect_GBM, y_train, cv=5, scoring = 'neg_mean_squared_error')).mean()\n",
    "            MSE_list_GB.append(MSE)\n",
    "            a = ne\n",
    "            b = md\n",
    "            c = (a,b)\n",
    "            dflist_GB.append(c)\n",
    "    return MSE_list_GB\n",
    "GB()\n",
    "df = pd.DataFrame(dflist_GB,columns=['NumEst', 'MaxDepth'])\n",
    "df['MSE']=MSE_list_GB\n",
    "df = df.sort_values(by ='MSE',ascending = True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pickle\n",
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Load the list of documents\n",
    "train_var = pd.read_csv(\"training_variants\")\n",
    "# Reading train\n",
    "train_text = pd.read_csv(\n",
    "    'training_text', sep=\"\\|\\|\", engine='python', header=None, skiprows=1, \n",
    "    names=[\"ID\",\"Text\"])\n",
    "\n",
    "# Use CountVectorizor to find three letter tokens, remove stop_words, \n",
    "# remove tokens that don't appear in at least 20 documents,\n",
    "# remove tokens that appear in more than 20% of the documents\n",
    "vect = CountVectorizer(min_df=20, max_df=0.2, stop_words='english', \n",
    "                       token_pattern='(?u)\\\\b\\\\w\\\\w\\\\w+\\\\b')\n",
    "# Fit and transform\n",
    "X = vect.fit_transform(train_text['Text'])\n",
    "\n",
    "# Convert sparse matrix to gensim corpus.\n",
    "corpus = gensim.matutils.Sparse2Corpus(X, documents_columns=False)\n",
    "\n",
    "# Mapping from word IDs to words (To be used in LdaModel's id2word parameter)\n",
    "id_map = dict((v, k) for k, v in vect.vocabulary_.items())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-931929e67992>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[1;31m# Your code here:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mldamodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgensim\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mldamodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mLdaModel\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnum_topics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m9\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mid2word\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mid_map\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mpasses\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m25\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m34\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, corpus, num_topics, id2word, distributed, chunksize, passes, update_every, alpha, eta, decay, offset, eval_every, iterations, gamma_threshold, minimum_probability, random_state, ns_conf, minimum_phi_value, per_word_topics)\u001b[0m\n\u001b[1;32m    333\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcorpus\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m             \u001b[0muse_numpy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mchunks_as_numpy\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0muse_numpy\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0minit_dir_prior\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mprior\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, corpus, chunksize, decay, offset, passes, update_every, eval_every, iterations, gamma_threshold, chunks_as_numpy)\u001b[0m\n\u001b[1;32m    640\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    641\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0meval_every\u001b[0m \u001b[1;32mand\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreallen\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mlencorpus\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mor\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk_no\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0meval_every\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumworkers\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 642\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlog_perplexity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtotal_docs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mlencorpus\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    643\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    644\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdispatcher\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mlog_perplexity\u001b[0;34m(self, chunk, total_docs)\u001b[0m\n\u001b[1;32m    525\u001b[0m         \u001b[0mcorpus_words\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msum\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcnt\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mdocument\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mchunk\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0m_\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcnt\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdocument\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    526\u001b[0m         \u001b[0msubsample_ratio\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m1.0\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mtotal_docs\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 527\u001b[0;31m         \u001b[0mperwordbound\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbound\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchunk\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msubsample_ratio\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msubsample_ratio\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m/\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0msubsample_ratio\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mcorpus_words\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    528\u001b[0m         logger.info(\"%.3f per-word bound, %.1f perplexity estimate based on a held-out corpus of %i documents with %i words\" %\n\u001b[1;32m    529\u001b[0m                     (perwordbound, np.exp2(-perwordbound), len(chunk), corpus_words))\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36mbound\u001b[0;34m(self, corpus, gamma, subsample_ratio)\u001b[0m\n\u001b[1;32m    732\u001b[0m                 \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdebug\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"bound: at document #%i\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    733\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mgamma\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 734\u001b[0;31m                 \u001b[0mgammad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    735\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    736\u001b[0m                 \u001b[0mgammad\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgamma\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\gensim\\models\\ldamodel.py\u001b[0m in \u001b[0;36minference\u001b[0;34m(self, chunk, collect_sstats)\u001b[0m\n\u001b[1;32m    441\u001b[0m             \u001b[1;31m# phinorm is the normalizer.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    442\u001b[0m             \u001b[1;31m# TODO treat zeros explicitly, instead of adding 1e-100?\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m--> 443\u001b[0;31m             \u001b[0mphinorm\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mexpElogthetad\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexpElogbetad\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1e-100\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    444\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m    445\u001b[0m             \u001b[1;31m# Iterate between gamma and phi until convergence\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Use the gensim.models.ldamodel.LdaModel constructor to estimate \n",
    "# LDA model parameters on the corpus, and save to the variable `ldamodel`\n",
    "\n",
    "# Your code here:\n",
    "ldamodel = gensim.models.ldamodel.LdaModel(corpus, num_topics=9, id2word=id_map, passes=25, random_state=34)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
